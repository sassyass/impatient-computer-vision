{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "<a href=\"https://colab.research.google.com/github/thesteve0/impatient-computer-vision/blob/main/3_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Embeddings\n",
    "\n",
    "There has been a lot of discussion about embeddings, and with good reason. Not only can they be used to power similarity search but they can also reveal fundamental relationships in our unstructured data. And there has been burgoneoning research showing how embeddings can often be used to replace the original data for many types of analysis - allowing for amazing data compression while retaining many of the original characterstics of the data.\n",
    "\n",
    "We are going to load the dataset from the end of the last notebook. The labels from the classifiers will become"
   ],
   "id": "4da7d37769d9f518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from math import trunc\n",
    "\n",
    "from generator3.core import progress\n",
    "!pip install fiftyone==1.4.1 torch torchvision umap-learn\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "name = \"our-photos\"\n",
    "dir = \"/content/drive/MyDrive/impatient-cv/flickr-as-pokemon\"\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    name=name\n",
    ")\n",
    "\n",
    "print(dataset)"
   ],
   "id": "f9d6e206e22fef4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using ResNet18 trained on \"typical data\"\n",
    "\n",
    "We are going to calculate the embeddings using the same ResNet18 model from the FiftyOne Model Zoo that we used for classification.\n",
    "\n",
    "I will let you in on a little secret, embeddings are the output from the final layer of the neural network before the classification layer. For most models,\n",
    "if you take off the last layer of the model and just take the output vector this is the embeddings. Actually, you could take output from any layer of the neural network but we almost always just take the output from the next to last layer.\n",
    "\n",
    "In the case of ResNet18 the output for embeddings is a 512 dimensional vector that should contain all the important \"features\" of the original data. The features that the model captures are a by-product of both the architecture of the model and the training data. These 512 numbers are the coordinates of this original image in a 512 dimensional space\n",
    "\n",
    "We can't visualize 512 dimensions, so we will use a dimension reduction method UMAP that tries to retain the \"closeness\" from the 512 dimensional space while reducing down to 2 dimensions. After we finish with dimension reduction we can then see how images are related in a 2 dimensional space. Because we are using a model from the FiftyOne Zoo we can calculate the embeddings and do dimension reduction all in one step with a `compute_visualization` method in the FiftyOne Brain library."
   ],
   "id": "bc89efedc0b5ffdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T05:28:03.590404Z",
     "start_time": "2025-05-08T05:28:03.587354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "resnet18_in = foz.load_zoo_model(\"resnet18-imagenet-torch\")\n",
    "\n",
    "# compute visualization\n",
    "\n",
    "fob.compute_visualization(\n",
    "    dataset=dataset,\n",
    "    model=resnet18_in,\n",
    "    embeddings=\"resnet18_in_embed\", # field name to store the embeddings\n",
    "    brain_key= \"resnet18_in_embed\", # run name for this brain method call\n",
    "    progres=True,\n",
    "    num_workers=4, # next two only applicable to a PyTorch model\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Now visualize - I will walk us through it in the app\n",
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url\n",
    "\n",
    "# session.show()\n"
   ],
   "id": "c348ce23b5ab7365",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Embeddings from our Pokemon Classification Model\n",
    "\n",
    "Since our ResNet18 model trained on Pokemon is not in the model zoo, we have to do a bit more work to visualize them. First we will calculate the embeddings and associate them with the samples, and then we will call `compute_visualization` passing in the embeddings.\n",
    "Once that is finished we can compare how well the different embeddings do in telling us \"interesting\" things about our photos."
   ],
   "id": "b2caec1cf1a16c2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.v2 as T\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model state dict using pickle\n",
    "with open('/content/drive/MyDrive/impatient-cv/pokemon-classification-model.pt', 'rb') as f:\n",
    "    state_dict = pickle.load(f)\n",
    "\n",
    "# Create a modified ResNet18 to extract embeddings\n",
    "class ResNetEmbedding(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super().__init__()\n",
    "        # Get all layers except the final fully connected layer\n",
    "        self.features = torch.nn.Sequential(*list(original_model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # Flatten the output to get the embedding vector\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "# Create base ResNet18 model with no pre-trained weights\n",
    "base_model = models.resnet18(weights=None)\n",
    "base_model.fc = torch.nn.Linear(base_model.fc.in_features, 150)  # Match original model\n",
    "\n",
    "# Clean state dict keys with dict comprehension\n",
    "if any(k.startswith('model.model.') for k in state_dict):\n",
    "    state_dict = {k.replace('model.model.', ''): v for k, v in state_dict.items()}\n",
    "elif any(k.startswith('model.') for k in state_dict):\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "# Load the state dict into the base model\n",
    "base_model.load_state_dict(state_dict)\n",
    "\n",
    "# Create the embedding model from the base model\n",
    "embedding_model = ResNetEmbedding(base_model)\n",
    "embedding_model.to(device)\n",
    "embedding_model.eval()\n",
    "\n",
    "# Preprocessing transforms\n",
    "transform = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.RGB(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "])\n",
    "\n",
    "# Custom dataset for parallel loading\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, sample_ids, filepaths, transform=None):\n",
    "        self.sample_ids = sample_ids\n",
    "        self.filepaths = filepaths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.sample_ids[idx]\n",
    "        filepath = self.filepaths[idx]\n",
    "\n",
    "        image = Image.open(filepath).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return sample_id, image\n",
    "\n",
    "# Extract sample IDs and filepaths\n",
    "sample_ids = dataset.values(\"id\")\n",
    "filepaths = dataset.values(\"filepath\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "pokemon_dataset = PokemonDataset(sample_ids, filepaths, transform)\n",
    "dataloader = DataLoader(\n",
    "    pokemon_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Dictionary to store all embeddings\n",
    "all_embeddings = {}\n",
    "\n",
    "# Process batches to extract embeddings\n",
    "print(\"Extracting embeddings...\")\n",
    "with torch.inference_mode():\n",
    "    for batch_ids, images in tqdm(dataloader):\n",
    "        # Move images to device\n",
    "        images = images.to(device, non_blocking=True)\n",
    "\n",
    "        # Extract embeddings with mixed precision\n",
    "        with torch.autocast(device_type='cuda', enabled=True):\n",
    "            batch_embeddings = embedding_model(images)\n",
    "\n",
    "        # Get embeddings from GPU\n",
    "        batch_embeddings_cpu = batch_embeddings.cpu().numpy()\n",
    "\n",
    "        # Store embeddings in dictionary\n",
    "        for i, sample_id in enumerate(batch_ids):\n",
    "            all_embeddings[sample_id] = batch_embeddings_cpu[i].tolist()\n",
    "\n",
    "# Convert dictionary to ordered list matching the dataset order\n",
    "embeddings_list = [all_embeddings[sample_id] for sample_id in sample_ids]\n",
    "\n",
    "# Update all samples in a single batch operation\n",
    "dataset.set_values(\"resnet18_pm_embed\", embeddings_list)\n",
    "\n",
    "# Save dataset\n",
    "dataset.save()\n",
    "\n",
    "print(\"Embeddings extraction complete and stored as 'rn18_pm_embeddings'\")"
   ],
   "id": "f4ffc476522e54a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have saved the embedding on the samples, we can call `compute_visualization` for pre-computed embeddings",
   "id": "b969722ba387fc2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fob.compute_visualization(\n",
    "    dataset=dataset,\n",
    "    embeddings=\"resnet18_pm_embed\", # field name to store the embeddings\n",
    "    brain_key= \"resnet18_pm_embed\", # run name for this brain method call\n",
    "    progres=True\n",
    ")\n",
    "\n",
    "session.refresh()"
   ],
   "id": "51c93789f57bf95d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## One final embedding view\n",
    "\n",
    "You might have noticed there was a brain run that was titled `openclip_embed`. OpenClip is a multimodal model that was trained on image and descriptions - this allows the model to respond to both:\n",
    "1. Image prompts - find images like this image\n",
    "2. Text prompts - \"Photos of cats\"\n",
    "\n",
    "Because it has knowledge of human text associated with the image, the model produces embeddings that are more closely aligned with human \"semantic\" understanding of an image.\n",
    "\n",
    "If we go back to the app and pull up the brain run for these embeddings, you can see the clusters are more closely related to human concepts."
   ],
   "id": "3bc3c7f19729d116"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "session.refresh()",
   "id": "add03e29f30b134f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary and take aways\n",
    "\n",
    "You have gotten a taste of embeddings - there really is a lot more to learn here. I highly encourage you to play with these more after the workshop.\n",
    "Similar to Classification, the data the model is trained up can have a significant impact on the embeddings it calculates. Be aware of this relationship and pay attention not only to model architecture but the training data.\n",
    "\n",
    "Looking at embeddings can help with:\n",
    "1. Quality of your ground truth or annotations. You can use the 2D embedding space to examine both the distribution of your images and how well your annotations sample the universe of potential images\n",
    "2. Assessing the quality of your models ability to understand your data. The cleaner the clustering of images into predicted classes the \"better\" the model is doing at distinguishing the features you care about in the data.\n",
    "\n",
    "One final little secret. You can actually \"concatenate\" embeddings together before you do dimension reduction. You might ask \"why would I do this\"? Well remember, different model architectures and different training data creates models that embed different features in the data. By combining them before dimension reduction, the dimension reduction technique will use information from BOTH embeddings to determine the 2d coordinates. If you combined our ResNet embeddings, which are sensitive to structure in the images, with the OpenClip embeddings, which are sensitive to human semantics, you end up with 2D points that bring more human semantics to the structural features of the data. I will leave it up to you to figure out how to do this. It really is quite interesting to see what happens to the plots.\n",
    "\n",
    "Alright, time to move on to a computer vision task a bit more sophisticated than Classification - Object Detection!"
   ],
   "id": "de9b1f6179143007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\"",
   "id": "cbbc3a9643f91a50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
