{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/thesteve0/impatient-computer-vision/blob/main/4_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "# Object Detection\n",
    "\n",
    "In Classification we try to come up with one label for the entire image. Now we are going to look at a more involved technique, Object Detection.  With Object Detection there are actually two tasks being accomplished:\n",
    "1. The model has to find all the \"things\" in the image\n",
    "2. Then it has to determine what those \"things\" actually are\n",
    "\n",
    "Typically, the models will surround the object with a bounding rectangle and then apply a label to the rectangle. There are some models that can also produce oriented rectangle which is helpful for use cases like drone imagery. Not only can you detect the car but you can tell its direction.\n",
    "\n",
    "Before we get started go ahead and run the housekeeping again. Once you have mapped the drive we will go back to the slides and discuss Object Detection"
   ],
   "id": "fd5687a0a48ad5b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install fiftyone==1.4.1 torch torchvision umap-learn\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "name = \"our-photos\"\n",
    "dir = \"/content/drive/MyDrive/impatient-cv/flickr-labeled\"\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    name=name\n",
    ")\n",
    "\n",
    "print(dataset)"
   ],
   "id": "f9d6e206e22fef4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running a model\n",
    "\n",
    "For the rest of the notebooks we are not going to dig in on custom models, we will just be using models from the FiftyOne Zoo. As much as possible, I will try to select a fairly typical model. Remember, the point is to give you a broad overview so you have a place to start when you dig in on the more complicated models.\n",
    "\n",
    "For this task we will use a Faster R-CNN that has a ResNet50 backbone - [faster-rcnn-resnet50-fpn-coco-torch](https://docs.voxel51.com/model_zoo/models.html#faster-rcnn-resnet50-fpn-coco-torch). This means the Faster R-CNN model uses the resnet model inside of its neural network, using it for classification of the detected objects. This model was trained on the [COCO dataset](https://cocodataset.org/#home) which is a more modern dataset but still similar in focus to ImageNet."
   ],
   "id": "9119b48af77b567"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model = foz.load_zoo_model(\"faster-rcnn-resnet50-fpn-coco-torch\")\n",
    "\n",
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"objects_detected\",\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "sample = dataset.first()\n",
    "sample"
   ],
   "id": "262c57ad3be88047",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Viewing in the application\n",
    "Now let's bring up the results in the application. You will see that, for some objects, we will get multiple detections. We can discuss how to handle those while we look at the data."
   ],
   "id": "4bd532d5adf5e0f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url\n",
    "\n",
    "# session.show()"
   ],
   "id": "4e811813a9f57b65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embedding the detections\n",
    "\n",
    "In the same way embeddings of our images were helpful for exploring and assessing our classifications, we can do the same with each of the individual detections. In this case we don't want all the candidate detections to be embedded, instead we will just use the high confidence detections. We will filter the data and then pass that to `compute_visualization`."
   ],
   "id": "a5310121d38f5e41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "high_confidence_data = dataset.filter_labels(\"objects_detected\", F(\"confidence\") > 0.7)\n",
    "\n",
    "# Load the model for calculating embeddings\n",
    "resnet18_in = foz.load_zoo_model(\"resnet18-imagenet-torch\")\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    samples=high_confidence_data,\n",
    "    patches_field=\"objects_detected\",\n",
    "    model=resnet18_in,\n",
    "    brain_key=\"objects_detected\",\n",
    "    embeddings=\"emb_objects_detected\",\n",
    "    progres=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "session.refresh()"
   ],
   "id": "1ed1d2ce60f38ebe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Wrap up\n",
    "\n",
    "In this notebook we went from trying to classify the whole image into finding objects of interest in the photograph and then classifying the object. For our original question, \"Is there a person in the image\", this type of output would have been much better suited than a classification model.\n",
    "In this case we could just iterate through the detections and if there was a human detection with a high confidence we could say the image contained a person.\n",
    "\n",
    "You may ask what is \"high confidence?\" Excellent question, and unfortunately the answer is \"it depends.\" There is usually a tradeoff between:\n",
    "1. Set the confidence high, which eliminates false positives but may omit false negatives\n",
    "2. Set the confidence low, which will include more true images but may also now include false positives\n",
    "\n",
    "The only way to handle this is to experiment and determine a threshold that works well for your intended application. To understand more you can read about the tradeoff between [precision and recall](https://spotintelligence.com/2024/09/11/precision-and-recall/)\n",
    "\n",
    "With our next notebook we are going to go even more fine-grained with our predicitions and try to predict every pixel in the image - time for Segmentation!\n",
    "\n",
    "[Segmentation](https://github.com/thesteve0/impatient-computer-vision/blob/main/5_segmentation.ipynb)"
   ],
   "id": "bbcb6ec6ac1e43a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
