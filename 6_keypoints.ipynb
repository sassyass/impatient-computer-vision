{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/thesteve0/impatient-computer-vision/blob/main/6_keypoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Keypoint Detection\n",
    "\n",
    "Alright, now we are going to do something a bit more specialized and, I think, amazing - keypoint detection. With this technique, the model detects key points in the objects and identifies or tracks them. An example of keypoints is landmarks on the human face like eyes, ears, nose, mouth, chin.... While we will only be doing this on still 2D images, you can doing this on 3D images, and on movies as well. Once you have identitied keypoints there is all sorts of downstream analysis you can do such, pose estimation, emotions, attention direction, and many other use cases. This tecnhique can be used beyond humans with it being used for animals, or any key identifying objects you want to track.\n",
    "\n",
    "Let's set up the house:"
   ],
   "id": "748d31ef21343271"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install fiftyone==1.4.1 torch torchvision ultralytics\n",
    "\n",
    "import fiftyone as fo\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "name = \"our-photos\"\n",
    "dir = \"/content/drive/MyDrive/impatient-cv/flickr-labeled\"\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    name=name\n",
    ")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download and running the model\n",
    "\n",
    "Setting up a keypoint model is a little more involved if you want to leverage all its value. Out of the box will identify points it has been trained to detect. But quite often, you want to label those points and then you want to connect points to form a \"skeleton.\"\n",
    "\n",
    "This time, instead of downloading a model from the Zoo, we will use one of the [Yolo models](https://docs.ultralytics.com/tasks/pose/#models) provided by Ultralytics. They come in many different sizes depending on your needs for speed versus accuracy, we will go with the middle of the road - `YOLO11m-pose`. FiftyOne comes with an [out-of-the-box integration](https://docs.voxel51.com/integrations/ultralytics.html) with Ultralytics, which greatly reduces the overhead of bringing in the model.\n"
   ],
   "id": "35bd2c7ff216ca15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "model = YOLO(\"yolo11m-pose.pt\")\n",
    "\n",
    "dataset.apply_model(model, label_field=\"keypoints\")\n",
    "\n",
    "dataset.default_skeleton = fo.KeypointSkeleton(\n",
    "    labels=[\n",
    "        \"nose\", \"left eye\", \"right eye\", \"left ear\", \"right ear\",\n",
    "        \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\",\n",
    "        \"left wrist\", \"right wrist\", \"left hip\", \"right hip\",\n",
    "        \"left knee\", \"right knee\", \"left ankle\", \"right ankle\",\n",
    "    ],\n",
    "    edges=[\n",
    "        [11, 5, 3, 1, 0, 2, 4, 6, 12],\n",
    "        [9, 7, 5, 6, 8, 10],\n",
    "        [15, 13, 11, 12, 14, 16],\n",
    "    ],\n",
    ")\n",
    "\n",
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url\n",
    "\n",
    "\n"
   ],
   "id": "2c75af678731837d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
